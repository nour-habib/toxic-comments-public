<h1>Toxic Comment Classification</h1>

This project tests different machine learning models to classify any type of abusive, offensive comments. The training data was taken from Wikipedia's comment section and manually tagged for toxicity. The various models that were tested are LSTM, Naive Bayes, CNN, RNN and GRU. LSTM was the best performing model for classifying toxic comments.

<img src="https://github.com/nour-habib/toxic-comments-public/blob/main/images/accuracy.png">
<img src="https://github.com/nour-habib/toxic-comments-public/blob/main/images/f1.png" width="600">
<img src="https://github.com/nour-habib/toxic-comments-public/blob/main/images/percision.png" width="400">
<img src="https://github.com/nour-habib/toxic-comments-public/blob/main/images/recall.png" width="400">
